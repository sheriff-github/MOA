{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10799942,"sourceType":"datasetVersion","datasetId":6703032}],"dockerImageVersionId":30886,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# First, let's check currently installed packages\nimport pkg_resources\nimport subprocess\nimport sys\n\ndef get_installed_packages():\n    \"\"\"Get list of installed packages and their versions\"\"\"\n    installed_packages = pkg_resources.working_set\n    return {i.key: i.version for i in installed_packages}\n\ndef check_and_install_packages():\n    \"\"\"Check and install required packages\"\"\"\n    required_packages = {\n        'pandas': '1.2.4',\n        'numpy': '1.19.5',\n        'scikit-learn': '0.24.2',\n        'xgboost': '1.4.2',\n        'seaborn': '0.11.1',\n        'matplotlib': '3.4.2',\n        'plotly': '5.1.0',\n        'joblib': '1.0.1'\n    }\n    \n    installed = get_installed_packages()\n    \n    print(\"Checking required packages...\")\n    packages_to_install = []\n    \n    for package, version in required_packages.items():\n        if package not in installed:\n            print(f\"{package} not found - will install version {version}\")\n            packages_to_install.append(f\"{package}=={version}\")\n        else:\n            print(f\"{package} found - version {installed[package]}\")\n    \n    if packages_to_install:\n        print(\"\\nInstalling missing packages...\")\n        for package in packages_to_install:\n            try:\n                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n                print(f\"Successfully installed {package}\")\n            except subprocess.CalledProcessError:\n                print(f\"Failed to install {package}\")\n    \n    print(\"\\nFinal package versions:\")\n    installed = get_installed_packages()\n    for package in required_packages:\n        print(f\"{package}: {installed.get(package, 'Not installed')}\")\n\ndef check_gpu_availability():\n    \"\"\"Check if GPU is available\"\"\"\n    try:\n        import torch\n        print(\"\\nGPU Availability:\")\n        print(f\"CUDA available: {torch.cuda.is_available()}\")\n        if torch.cuda.is_available():\n            print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n    except ImportError:\n        try:\n            import tensorflow as tf\n            print(\"\\nGPU Availability:\")\n            print(f\"GPU devices: {tf.config.list_physical_devices('GPU')}\")\n        except ImportError:\n            print(\"\\nNeither PyTorch nor TensorFlow is installed - cannot check GPU availability\")\n\n# Run the checks\nif __name__ == \"__main__\":\n    print(\"=== Package Setup and Verification ===\\n\")\n    check_and_install_packages()\n    check_gpu_availability()\n    \n    # Check memory availability\n    import psutil\n    memory = psutil.virtual_memory()\n    print(f\"\\nAvailable Memory: {memory.available / (1024 * 1024 * 1024):.2f} GB\")\n    print(f\"Total Memory: {memory.total / (1024 * 1024 * 1024):.2f} GB\")","metadata":{"_uuid":"a9bc0aea-d659-4d26-b545-93ec3386412d","_cell_guid":"f119c7d4-8935-4f07-b3cd-14a0cc873a7f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T05:02:13.758329Z","iopub.execute_input":"2025-03-02T05:02:13.758610Z","iopub.status.idle":"2025-03-02T05:02:19.047215Z","shell.execute_reply.started":"2025-03-02T05:02:13.758582Z","shell.execute_reply":"2025-03-02T05:02:19.046213Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"<ipython-input-2-c96c156430a9>:2: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  import pkg_resources\n","output_type":"stream"},{"name":"stdout","text":"=== Package Setup and Verification ===\n\nChecking required packages...\npandas found - version 2.2.3\nnumpy found - version 1.26.4\nscikit-learn found - version 1.2.2\nxgboost found - version 2.0.3\nseaborn found - version 0.12.2\nmatplotlib found - version 3.7.5\nplotly found - version 5.24.1\njoblib found - version 1.4.2\n\nFinal package versions:\npandas: 2.2.3\nnumpy: 1.26.4\nscikit-learn: 1.2.2\nxgboost: 2.0.3\nseaborn: 0.12.2\nmatplotlib: 3.7.5\nplotly: 5.24.1\njoblib: 1.4.2\n\nGPU Availability:\nCUDA available: True\nGPU device: Tesla T4\n\nAvailable Memory: 29.88 GB\nTotal Memory: 31.35 GB\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():\n    print(\"GPU is available!\")\n    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n    print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")  # Prints the name of the first GPU\nelse:\n    print(\"GPU is not available.\")\n\n# If you have multiple GPUs and want to use them with XGBoost, you'll need to configure XGBoost appropriately.\n# XGBoost will automatically use all available GPUs by default if built with GPU support.\n# If you need more fine-grained control, you can set the 'n_gpus' parameter in your XGBoost training parameters.","metadata":{"_uuid":"59216b1b-f45c-44d6-93ea-4c2d984db4c8","_cell_guid":"cb702a05-2e14-4a69-a319-d19a2ea11e24","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T05:02:19.048134Z","iopub.execute_input":"2025-03-02T05:02:19.048512Z","iopub.status.idle":"2025-03-02T05:02:19.054015Z","shell.execute_reply.started":"2025-03-02T05:02:19.048489Z","shell.execute_reply":"2025-03-02T05:02:19.053163Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"GPU is available!\nNumber of GPUs: 2\nCurrent GPU: Tesla T4\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Module 1: Imports and Setup\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.metrics import log_loss, roc_auc_score, precision_recall_curve, auc, roc_curve\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport xgboost as xgb\nfrom joblib import Parallel, delayed\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport time\nimport logging\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots","metadata":{"_uuid":"d254c93b-5b78-466b-a83f-5bd2a265a254","_cell_guid":"15074059-1a55-4279-b561-a3eb01716949","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T05:02:19.054810Z","iopub.execute_input":"2025-03-02T05:02:19.055079Z","iopub.status.idle":"2025-03-02T05:02:21.881099Z","shell.execute_reply.started":"2025-03-02T05:02:19.055049Z","shell.execute_reply":"2025-03-02T05:02:21.880385Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Module 2: Data Loading and Initial Processing\ndef load_and_preprocess_data(train_features_path, train_targets_path, test_features_path):\n    \"\"\"\n    Load and preprocess the data files\n    \"\"\"\n    print(\"Loading data...\")\n    train_features = pd.read_csv(\"/kaggle/input/my-other-dataset/train_features.csv\")\n    train_targets_scored = pd.read_csv(\"/kaggle/input/my-other-dataset/train_targets_scored.csv\")\n    test_features = pd.read_csv(\"/kaggle/input/my-other-dataset/test_features.csv\")\n    \n    # Separate features and targets\n    X = train_features.drop(columns=['sig_id'])\n    y = train_targets_scored.drop(columns=['sig_id'])\n    \n    # Handle control perturbations\n    control_mask = X['cp_type'] == 'ctrl_vehicle'\n    X = X[~control_mask]\n    y = y[~control_mask]\n    \n    return X, y, test_features, control_mask","metadata":{"_uuid":"626bd518-cb33-4add-8bd7-bb97351af62b","_cell_guid":"b5a67095-8da8-42ff-b6a2-4fa05900ff9f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T05:02:39.349506Z","iopub.execute_input":"2025-03-02T05:02:39.349811Z","iopub.status.idle":"2025-03-02T05:02:39.354944Z","shell.execute_reply.started":"2025-03-02T05:02:39.349787Z","shell.execute_reply":"2025-03-02T05:02:39.354026Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Module 3: Feature Engineering\ndef engineer_features(X):\n    \"\"\"\n    Perform feature engineering on the dataset\n    \"\"\"\n    print(\"Engineering features...\")\n    # Statistical features\n    X['g_mean'] = X.filter(regex='^g-').mean(axis=1)\n    X['g_std'] = X.filter(regex='^g-').std(axis=1)\n    X['c_mean'] = X.filter(regex='^c-').mean(axis=1)\n    X['c_std'] = X.filter(regex='^c-').std(axis=1)\n    \n    # Optional: Comment these out if they're causing issues\n    X['g_skew'] = X.filter(regex='^g-').skew(axis=1)\n    X['c_skew'] = X.filter(regex='^c-').skew(axis=1)\n    X['g_kurtosis'] = X.filter(regex='^g-').kurtosis(axis=1)\n    X['c_kurtosis'] = X.filter(regex='^c-').kurtosis(axis=1)\n    \n    # Drop cp_type and encode categoricals\n    X = X.drop(columns=['cp_type'])\n    X = pd.get_dummies(X, columns=['cp_time', 'cp_dose'], drop_first=True)\n    \n    return X","metadata":{"_uuid":"ab9496a8-c7c2-4ff7-84ec-db750bf6c15b","_cell_guid":"5e6688a6-aa36-420d-afb9-f9ef6ba9a118","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T05:02:50.045629Z","iopub.execute_input":"2025-03-02T05:02:50.045944Z","iopub.status.idle":"2025-03-02T05:02:50.051789Z","shell.execute_reply.started":"2025-03-02T05:02:50.045922Z","shell.execute_reply":"2025-03-02T05:02:50.051028Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Module 4: Visualization Functions\ndef plot_target_distribution(y, save_path='target_distribution.png'):\n    \"\"\"\n    Plot distribution of target variables\n    \"\"\"\n    plt.figure(figsize=(15, 6))\n    means = y.mean().sort_values(ascending=True)\n    plt.bar(range(len(means)), means)\n    plt.title('Distribution of Target Variables')\n    plt.xlabel('Target Index')\n    plt.ylabel('Mean Value')\n    plt.tight_layout()\n    plt.savefig(save_path)\n    plt.close()\n\ndef plot_feature_importance(feature_importance_df, top_n=20, save_path='feature_importance.png'):\n    \"\"\"\n    Plot top N most important features\n    \"\"\"\n    plt.figure(figsize=(12, 8))\n    sns.barplot(x='mean_importance', \n                y=feature_importance_df.index,\n                data=feature_importance_df.sort_values('mean_importance', ascending=True).tail(top_n))\n    plt.title(f'Top {top_n} Most Important Features')\n    plt.tight_layout()\n    plt.savefig(save_path)\n    plt.close()\n\ndef plot_roc_curves(y_true, y_pred, target_name, save_path=None):\n    \"\"\"\n    Plot ROC curve for a target\n    \"\"\"\n    fpr, tpr, _ = roc_curve(y_true, y_pred)\n    roc_auc = auc(fpr, tpr)\n    \n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(f'ROC Curve - {target_name}')\n    plt.legend(loc=\"lower right\")\n    \n    if save_path:\n        plt.savefig(save_path)\n    plt.close()","metadata":{"_uuid":"645218fd-559d-4b4e-8e39-7294c60dc8ed","_cell_guid":"e64a3ff0-0ef9-4482-b242-e853527622c8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T05:03:02.193348Z","iopub.execute_input":"2025-03-02T05:03:02.193641Z","iopub.status.idle":"2025-03-02T05:03:02.201177Z","shell.execute_reply.started":"2025-03-02T05:03:02.193615Z","shell.execute_reply":"2025-03-02T05:03:02.200281Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Module 5: Model Training and Evaluation\nclass ConstantPredictor:\n    def __init__(self, value):\n        self.value = float(value)\n    \n    def predict_proba(self, X):\n        n_samples = len(X)\n        probs = np.zeros((n_samples, 2))\n        probs[:, 0] = 1 - self.value\n        probs[:, 1] = self.value\n        return probs\n    \n    def predict(self, X):\n        return np.ones(len(X)) * self.value\n    \n    def fit(self, X, y):\n        return self\n\ndef evaluate_predictions(y_true, y_pred, target_name):\n    \"\"\"\n    Calculate various metrics for model evaluation\n    \"\"\"\n    metrics = {}\n    metrics['log_loss'] = log_loss(y_true, y_pred)\n    \n    # Convert probabilities to binary predictions using 0.5 threshold\n    y_pred_binary = (y_pred > 0.5).astype(int)\n    \n    # Calculate additional metrics\n    metrics['roc_auc'] = roc_auc_score(y_true, y_pred)\n    \n    # Precision-Recall AUC\n    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n    metrics['pr_auc'] = auc(recall, precision)\n    \n    # Classification report\n    report = classification_report(y_true, y_pred_binary, output_dict=True)\n    metrics.update({\n        'precision': report['1']['precision'],\n        'recall': report['1']['recall'],\n        'f1_score': report['1']['f1-score']\n    })\n    \n    # Confusion matrix\n    cm = confusion_matrix(y_true, y_pred_binary)\n    metrics['confusion_matrix'] = cm\n    \n    return metrics\n\ndef train_target_model(target, X_train, y_train, X_val, y_val, xgb_params):\n    \"\"\"\n    Train and evaluate model for a single target with GPU acceleration and multi-GPU support\n    \"\"\"\n    result = {\n        'target': target,\n        'metrics': None,\n        'model': None,\n        'predictions': None,\n        'feature_importance': None\n    }\n    \n    print(f\"\\nProcessing target: {target}\")\n    \n    # Check if target has variation\n    if len(np.unique(y_train[target])) == 1:\n        constant_value = float(y_train[target].iloc[0])\n        print(f\"Target {target} has only one class (constant value: {constant_value})\")\n        result['model'] = ConstantPredictor(constant_value)\n        result['predictions'] = np.ones(len(y_val)) * constant_value\n    else:\n        try:\n            # Create DMatrix for XGBoost\n            dtrain = xgb.DMatrix(X_train, label=y_train[target])\n            dval = xgb.DMatrix(X_val, label=y_val[target])\n            \n            # Calculate scale_pos_weight for imbalanced data\n            pos_weight = (y_train[target] == 0).sum() / max(1, (y_train[target] == 1).sum())\n            local_params = xgb_params.copy()\n            local_params['scale_pos_weight'] = pos_weight\n            \n            # Train the model\n            print(f\"Training target {target} on GPU(s)...\")\n            model = xgb.train(\n                local_params,\n                dtrain,\n                num_boost_round=200,\n                evals=[(dval, 'eval')],\n                early_stopping_rounds=20,\n                verbose_eval=False\n            )\n            \n            # Store the trained model and predictions\n            result['model'] = model\n            val_preds = model.predict(dval)\n            result['predictions'] = val_preds\n            \n            # Calculate evaluation metrics\n            result['metrics'] = evaluate_predictions(y_val[target], val_preds, target)\n            \n            # Get feature importance\n            result['feature_importance'] = model.get_score(importance_type='gain')\n            \n            # Generate and save ROC curve\n            # Need to import roc_curve from sklearn.metrics\n            try:\n                from sklearn.metrics import roc_curve\n                fpr, tpr, _ = roc_curve(y_val[target], val_preds)\n                plt.figure(figsize=(8, 6))\n                plt.plot(fpr, tpr, label=f'ROC curve (AUC = {result[\"metrics\"][\"roc_auc\"]:.2f})')\n                plt.plot([0, 1], [0, 1], 'k--')\n                plt.xlim([0.0, 1.0])\n                plt.ylim([0.0, 1.05])\n                plt.xlabel('False Positive Rate')\n                plt.ylabel('True Positive Rate')\n                plt.title(f'ROC Curve - {target}')\n                plt.legend(loc=\"lower right\")\n                plt.savefig(f'roc_curves/{target}_roc.png')\n                plt.close()\n            except Exception as e:\n                print(f\"Error creating ROC curve for {target}: {str(e)}\")\n            \n        except Exception as e:\n            # Handle errors during training\n            print(f\"Error in training target {target}: {str(e)}\")\n            majority_class = float(y_train[target].mode()[0])\n            result['model'] = ConstantPredictor(majority_class)\n            result['predictions'] = np.ones(len(y_val)) * majority_class\n    \n    return result","metadata":{"_uuid":"f84ab309-f63e-4fd9-9ba1-7f9c3dbf91dc","_cell_guid":"bc2c593c-9f17-4a4b-89bc-d3dba32a3cef","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T05:03:07.261754Z","iopub.execute_input":"2025-03-02T05:03:07.262092Z","iopub.status.idle":"2025-03-02T05:03:07.274825Z","shell.execute_reply.started":"2025-03-02T05:03:07.262061Z","shell.execute_reply":"2025-03-02T05:03:07.273754Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Module 6: Main Pipeline\ndef run_moa_pipeline(train_features_path, train_targets_path, test_features_path):\n    \"\"\"\n    Run the complete MoA prediction pipeline\n    \"\"\"\n    # Create directories for outputs\n    os.makedirs('plots', exist_ok=True)\n    os.makedirs('roc_curves', exist_ok=True)\n    os.makedirs('metrics', exist_ok=True)\n    \n    # Load and preprocess data\n    X, y, test_features, control_mask = load_and_preprocess_data(\n        train_features_path, train_targets_path, test_features_path\n    )\n    \n    # Engineer features\n    X = engineer_features(X)\n    \n    # Plot initial target distribution\n    plot_target_distribution(y, save_path='plots/target_distribution.png')\n    \n    # Split data\n    X_train, X_val, y_train, y_val = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y.iloc[:, 0]\n    )\n    \n    # Scale features\n    scaler = RobustScaler()\n    numerical_cols = [col for col in X.columns if col.startswith(('g-', 'c-'))]\n    X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n    X_val[numerical_cols] = scaler.transform(X_val[numerical_cols])\n    \n    # XGBoost parameters - FIX: Removed duplicate tree_method\n    xgb_params = {\n        'objective': 'binary:logistic',\n        'eval_metric': 'logloss',\n        'tree_method': 'gpu_hist',  # Use GPU-accelerated tree method\n        'max_depth': 6,\n        'learning_rate': 0.05,\n        'subsample': 0.8,\n        'colsample_bytree': 0.8,\n        'min_child_weight': 3,\n        'gamma': 0.1,\n        'reg_alpha': 0.1,\n        'reg_lambda': 1.0,\n        'verbosity': 3,\n        'n_gpus': 2  # Use both GPUs\n    }\n    \n    # Train models in parallel\n    start_time = time.time()\n    results = Parallel(n_jobs=-1, verbose=10)(\n        delayed(train_target_model)(\n            target, X_train, y_train, X_val, y_val, xgb_params\n        ) for target in y_train.columns\n    )\n    \n    # Process results and create visualizations\n    process_and_visualize_results(results, X_train.columns)\n    \n    # Prepare and make predictions on test set\n    test_predictions = predict_test_set(\n        test_features, control_mask, X_train.columns, \n        numerical_cols, scaler, results\n    )\n    \n    # Save final results\n    save_final_results(test_predictions, test_features['sig_id'], results)\n    \n    print(f\"\\nTotal execution time: {(time.time() - start_time)/60:.2f} minutes\")","metadata":{"_uuid":"ac31a2bd-f06b-46b3-b1a8-ae33ebea8611","_cell_guid":"fa2e86fb-717f-4c36-bfc7-cc66409f5c15","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T05:03:16.427242Z","iopub.execute_input":"2025-03-02T05:03:16.427569Z","iopub.status.idle":"2025-03-02T05:03:16.434845Z","shell.execute_reply.started":"2025-03-02T05:03:16.427541Z","shell.execute_reply":"2025-03-02T05:03:16.434117Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Module 7: Results Processing and Visualization\n\ndef compile_feature_importance(results, feature_columns):\n    \"\"\"\n    Compile feature importance scores from all trained models\n    \"\"\"\n    feature_importance = []\n\n    for result in results:\n        target = result['target']\n        importance = result['feature_importance']\n        \n        if importance:\n            # Normalize importance values\n            total_importance = sum(importance.values())\n            normalized_importance = {k: v / total_importance for k, v in importance.items()}\n            \n            # Append to the list\n            for feature, score in normalized_importance.items():\n                feature_importance.append({\n                    'feature': feature,\n                    'importance': score,\n                    'target': target\n                })\n    \n    # Create a DataFrame\n    feature_importance_df = pd.DataFrame(feature_importance)\n    \n    # Aggregate importance across all targets\n    aggregated_importance = (\n        feature_importance_df\n        .groupby('feature')['importance']\n        .mean()\n        .reset_index()  # Reset index to avoid issues\n        .rename(columns={'importance': 'mean_importance'})\n    )\n    \n    # Sort by mean importance\n    aggregated_importance = aggregated_importance.sort_values(by='mean_importance', ascending=False)\n    \n    return aggregated_importance\n\n\ndef plot_metrics_distribution(metrics_df, save_dir):\n    \"\"\"\n    Create distribution plots for various metrics\n    \"\"\"\n    # List of metrics to visualize\n    metrics_to_plot = ['roc_auc', 'pr_auc', 'log_loss', 'f1_score']\n    \n    # Create a directory for saving plots if it doesn't exist\n    os.makedirs(save_dir, exist_ok=True)\n    \n    for metric in metrics_to_plot:\n        if metric in metrics_df.columns:\n            plt.figure(figsize=(10, 6))\n            sns.histplot(data=metrics_df[metric], bins=50, kde=True)\n            plt.title(f'Distribution of {metric}')\n            plt.xlabel(metric)\n            plt.ylabel('Count')\n            plt.tight_layout()\n            plt.savefig(f'{save_dir}/{metric}_distribution.png')\n            plt.close()\n\n\ndef plot_feature_importance(feature_importance_df, top_n=20, save_path='feature_importance.png'):\n    \"\"\"\n    Plot top N most important features\n    \"\"\"\n    plt.figure(figsize=(12, 8))\n    \n    # Select top N features\n    top_features = feature_importance_df.head(top_n)\n    \n    # Plot bar chart\n    sns.barplot(\n        x='mean_importance',\n        y='feature',  # Use the 'feature' column instead of index\n        data=top_features,\n        orient='h'\n    )\n    \n    plt.title(f'Top {top_n} Most Important Features')\n    plt.xlabel('Mean Importance')\n    plt.ylabel('Feature')\n    plt.tight_layout()\n    plt.savefig(save_path)\n    plt.close()\n\n\ndef process_and_visualize_results(results, feature_columns):\n    \"\"\"\n    Process results and create visualizations\n    \"\"\"\n    # Compile metrics\n    metrics_df = pd.DataFrame([\n        {\n            'target': r['target'],\n            **r['metrics']\n        } for r in results if r['metrics'] is not None\n    ])\n    \n    # Create metrics visualizations\n    plot_metrics_distribution(metrics_df, save_dir='plots')\n    \n    # Process feature importance\n    feature_importance_df = compile_feature_importance(results, feature_columns)\n    plot_feature_importance(feature_importance_df, save_path='plots/feature_importance.png')\n    \n    # Save detailed metrics\n    metrics_df.to_csv('metrics/detailed_metrics.csv', index=False)\n    feature_importance_df.to_csv('metrics/feature_importance.csv', index=False)","metadata":{"_uuid":"708011e5-1b97-43e2-9967-7def9d0ed307","_cell_guid":"514a88d1-007a-4f6f-89a3-e05f66491bb5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T05:03:29.117837Z","iopub.execute_input":"2025-03-02T05:03:29.118180Z","iopub.status.idle":"2025-03-02T05:03:29.128444Z","shell.execute_reply.started":"2025-03-02T05:03:29.118152Z","shell.execute_reply":"2025-03-02T05:03:29.127586Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Module 8: Test Set Prediction\nfrom sklearn.metrics import roc_curve  # Add this import\n\ndef predict_test_set(test_features, control_mask, train_columns, \n                    numerical_cols, scaler, results):\n    \"\"\"\n    Make predictions on the test set\n    \"\"\"\n    # Preprocess test features\n    test_features_processed = engineer_features(test_features[~control_mask].copy())\n    \n    # Align columns with training data\n    test_features_processed = align_test_features(\n        test_features_processed, train_columns\n    )\n    \n    # Scale features\n    test_features_processed[numerical_cols] = scaler.transform(\n        test_features_processed[numerical_cols]\n    )\n    \n    # Make predictions\n    predictions = make_test_predictions(\n        test_features_processed, results, test_features.index, control_mask\n    )\n    \n    return predictions","metadata":{"_uuid":"65960e0c-4680-4e06-a109-692dab1b3df1","_cell_guid":"fdcb5c96-ac5e-44d6-ae0d-be4ad5d446f0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T05:03:40.906263Z","iopub.execute_input":"2025-03-02T05:03:40.906555Z","iopub.status.idle":"2025-03-02T05:03:40.911401Z","shell.execute_reply.started":"2025-03-02T05:03:40.906532Z","shell.execute_reply":"2025-03-02T05:03:40.910438Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Module 9: Utility Functions\ndef align_test_features(test_features, train_columns):\n    \"\"\"\n    Align test features with training features\n    \"\"\"\n    missing_cols = set(train_columns) - set(test_features.columns)\n    for col in missing_cols:\n        test_features[col] = 0\n    \n    extra_cols = set(test_features.columns) - set(train_columns)\n    test_features = test_features.drop(columns=list(extra_cols))\n    \n    return test_features[train_columns]\n\ndef make_test_predictions(test_features, results, index, control_mask):\n    \"\"\"\n    Make predictions for test set\n    \"\"\"\n    predictions = pd.DataFrame(index=index, \n                             columns=[r['target'] for r in results], \n                             dtype=float)\n    \n    # Make predictions for non-control samples\n    for result in results:\n        target = result['target']\n        model = result['model']\n        \n        try:\n            if isinstance(model, xgb.Booster):\n                dtest = xgb.DMatrix(test_features)\n                predictions.loc[~control_mask, target] = model.predict(dtest)\n            else:\n                predictions.loc[~control_mask, target] = model.predict(test_features)\n        except Exception as e:\n            print(f\"Error predicting {target}: {str(e)}\")\n            if isinstance(model, ConstantPredictor):\n                predictions[target] = float(model.value)\n            else:\n                predictions[target] = 0.0\n    \n    # Set control predictions to 0\n    predictions.loc[control_mask, :] = 0.0\n    \n    # Fix: Use less aggressive clipping as per competition guidelines\n    # According to: max(min(p,1−10−15),10−15)\n    predictions = predictions.clip(1e-15, 1 - 1e-15)\n    \n    # Print prediction stats for verification\n    pred_values = predictions.values.flatten()\n    print(\"\\nPrediction Statistics After Clipping:\")\n    print(f\"Mean: {pred_values.mean():.6f}\")\n    print(f\"Min: {pred_values.min():.6f}\")\n    print(f\"Max: {pred_values.max():.6f}\")\n    print(f\"Values <0.001: {(pred_values < 0.001).sum()} ({(pred_values < 0.001).sum()/len(pred_values)*100:.2f}%)\")\n    \n    return predictions","metadata":{"_uuid":"847b667a-c07d-4e3f-b521-75ecf6114a9d","_cell_guid":"118c3256-c3ae-46dc-a525-2bca6efb3e64","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T05:03:54.050309Z","iopub.execute_input":"2025-03-02T05:03:54.050613Z","iopub.status.idle":"2025-03-02T05:03:54.058248Z","shell.execute_reply.started":"2025-03-02T05:03:54.050591Z","shell.execute_reply":"2025-03-02T05:03:54.057437Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Module 10: Final Results and Metrics Output\ndef save_final_results(predictions, sig_ids, results, output_dir='outputs'):\n    \"\"\"\n    Save final results and print detailed metrics\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Create submission file\n    submission = pd.DataFrame({'sig_id': sig_ids})\n    submission = pd.concat([submission, predictions], axis=1)\n    submission.to_csv(f'{output_dir}/submission.csv', index=False)\n    \n    # Compile and print detailed metrics\n    print(\"\\n=== Model Performance Metrics ===\")\n    \n    # Overall metrics\n    all_metrics = [r['metrics'] for r in results if r['metrics'] is not None]\n    metrics_summary = {\n        'roc_auc': [m['roc_auc'] for m in all_metrics if 'roc_auc' in m],\n        'pr_auc': [m['pr_auc'] for m in all_metrics if 'pr_auc' in m],\n        'log_loss': [m['log_loss'] for m in all_metrics if 'log_loss' in m],\n        'f1_score': [m['f1_score'] for m in all_metrics if 'f1_score' in m]\n    }\n    \n    print(\"\\nOverall Metrics:\")\n    for metric, values in metrics_summary.items():\n        print(f\"{metric}:\")\n        print(f\"  Mean: {np.mean(values):.4f}\")\n        print(f\"  Std:  {np.std(values):.4f}\")\n        print(f\"  Min:  {np.min(values):.4f}\")\n        print(f\"  Max:  {np.max(values):.4f}\")\n    \n    # Submission statistics\n    print(\"\\nSubmission Statistics:\")\n    print(f\"Number of samples in submission: {len(submission)}\")\n    print(f\"Number of MoA targets: {len(predictions.columns)}\")\n    \n    # Prediction value ranges\n    prediction_values = predictions.values.flatten()\n    print(\"\\nPrediction value ranges:\")\n    print(f\"Mean: {prediction_values.mean():.6f}\")\n    print(f\"Std: {prediction_values.std():.6f}\")\n    print(f\"Min: {prediction_values.min():.6f}\")\n    print(f\"Max: {prediction_values.max():.6f}\")\n    \n    print(\"\\nNumber of predictions by value range:\")\n    print(f\"0-0.001: {np.sum((prediction_values > 0) & (prediction_values <= 0.001))}\")\n    print(f\"0.001-0.01: {np.sum((prediction_values > 0.001) & (prediction_values <= 0.01))}\")\n    print(f\"0.01-0.1: {np.sum((prediction_values > 0.01) & (prediction_values <= 0.1))}\")\n    print(f\"0.1-0.5: {np.sum((prediction_values > 0.1) & (prediction_values <= 0.5))}\")\n    print(f\"0.5-1.0: {np.sum((prediction_values > 0.5) & (prediction_values <= 1.0))}\")\n    \n    # Validation checks\n    print(\"\\nValidation Checks:\")\n    print(\"Missing values:\", submission.isnull().sum().sum())\n    print(\"Infinite values:\", np.isinf(predictions.values).sum())\n    print(\"Values outside [0,1]:\", np.sum((predictions.values < 0) | (predictions.values > 1)))\n    \n    # Save detailed metrics per target\n    detailed_metrics = []\n    for result in results:\n        if result['metrics'] is not None:\n            metrics_dict = {\n                'target': result['target'],\n                **result['metrics']\n            }\n            detailed_metrics.append(metrics_dict)\n    \n    detailed_metrics_df = pd.DataFrame(detailed_metrics)\n    detailed_metrics_df.to_csv(f'{output_dir}/detailed_metrics.csv', index=False)\n    \n    # Print top and bottom performing targets\n    print(\"\\nTop 5 performing targets (by ROC-AUC):\")\n    print(detailed_metrics_df.nlargest(5, 'roc_auc')[['target', 'roc_auc', 'pr_auc', 'log_loss']])\n    \n    print(\"\\nBottom 5 performing targets (by ROC-AUC):\")\n    print(detailed_metrics_df.nsmallest(5, 'roc_auc')[['target', 'roc_auc', 'pr_auc', 'log_loss']])","metadata":{"_uuid":"2f72f118-1d7e-41b0-af05-9f8de89297e6","_cell_guid":"efe14ddb-84db-4241-8299-d0281f208451","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T05:03:57.294037Z","iopub.execute_input":"2025-03-02T05:03:57.294353Z","iopub.status.idle":"2025-03-02T05:03:57.304974Z","shell.execute_reply.started":"2025-03-02T05:03:57.294330Z","shell.execute_reply":"2025-03-02T05:03:57.304229Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Module 11: Main Execution\nif __name__ == \"__main__\":\n    # Set random seed for reproducibility\n    np.random.seed(42)\n    \n    # Define paths\n    TRAIN_FEATURES_PATH = '/kaggle/input/my-other-dataset/train_features.csv'\n    TRAIN_TARGETS_PATH = '/kaggle/input/my-other-dataset/train_targets_scored.csv'\n    TEST_FEATURES_PATH = '/kaggle/input/my-other-dataset/test_features.csv'\n    OUTPUT_DIR = '/kaggle/working/moa_outputs'  # Updated to Kaggle's writable directory\n    \n    # Create output directory\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    \n    # Configure logging\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.FileHandler(f'{OUTPUT_DIR}/pipeline.log'),\n            logging.StreamHandler()\n        ]\n    )\n    \n    try:\n        # Verify file paths before running the pipeline\n        if not os.path.exists(TRAIN_FEATURES_PATH):\n            raise FileNotFoundError(f\"Train features file not found at: {TRAIN_FEATURES_PATH}\")\n        if not os.path.exists(TRAIN_TARGETS_PATH):\n            raise FileNotFoundError(f\"Train targets file not found at: {TRAIN_TARGETS_PATH}\")\n        if not os.path.exists(TEST_FEATURES_PATH):\n            raise FileNotFoundError(f\"Test features file not found at: {TEST_FEATURES_PATH}\")\n        \n        # Run the complete pipeline\n        run_moa_pipeline(\n            TRAIN_FEATURES_PATH,\n            TRAIN_TARGETS_PATH,\n            TEST_FEATURES_PATH\n        )\n        \n        logging.info(\"Pipeline completed successfully!\")\n        \n    except Exception as e:\n        logging.error(f\"Pipeline failed with error: {str(e)}\")\n        raise","metadata":{"_uuid":"845cca06-f786-41d7-bf5a-676c8a7ab896","_cell_guid":"87377796-3397-4e19-81e8-bb76240998e9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T05:04:05.042721Z","iopub.execute_input":"2025-03-02T05:04:05.043039Z","iopub.status.idle":"2025-03-02T05:17:25.041589Z","shell.execute_reply.started":"2025-03-02T05:04:05.042983Z","shell.execute_reply":"2025-03-02T05:17:25.040556Z"},"_kg_hide-input":true,"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Loading data...\nEngineering features...\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   32.6s\n[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   50.7s\n[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  1.1min\n[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  1.6min\n[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  2.0min\n[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.6min\n[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:  3.1min\n[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  3.8min\n[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:  4.6min\n[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:  5.3min\n[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:  6.1min\n[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  7.1min\n[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:  8.1min\n[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  9.2min\n[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed: 10.2min\n[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 11.2min\n[Parallel(n_jobs=-1)]: Done 206 out of 206 | elapsed: 12.0min finished\n","output_type":"stream"},{"name":"stdout","text":"Engineering features...\n\nPrediction Statistics After Clipping:\nMean: 0.005170\nMin: 0.000000\nMax: 0.999925\nValues <0.001: 560780 (68.36%)\n\n=== Model Performance Metrics ===\n\nOverall Metrics:\nroc_auc:\n  Mean: 0.7207\n  Std:  0.1875\n  Min:  0.0557\n  Max:  1.0000\npr_auc:\n  Mean: 0.1293\n  Std:  0.2529\n  Min:  0.0001\n  Max:  1.0000\nlog_loss:\n  Mean: 0.0179\n  Std:  0.0202\n  Min:  0.0005\n  Max:  0.1087\nf1_score:\n  Mean: 0.1153\n  Std:  0.2518\n  Min:  0.0000\n  Max:  1.0000\n\nSubmission Statistics:\nNumber of samples in submission: 3982\nNumber of MoA targets: 206\n\nPrediction value ranges:\nMean: 0.005170\nStd: 0.033372\nMin: 0.000000\nMax: 0.999925\n\nNumber of predictions by value range:\n0-0.001: 560780\n0.001-0.01: 194036\n0.01-0.1: 59001\n0.1-0.5: 5695\n0.5-1.0: 780\n\nValidation Checks:\nMissing values: 0\nInfinite values: 0\nValues outside [0,1]: 0\n\nTop 5 performing targets (by ROC-AUC):\n                               target   roc_auc    pr_auc  log_loss\n34             atp_synthase_inhibitor  1.000000  1.000000  0.000454\n154              proteasome_inhibitor  0.999978  0.999296  0.002101\n178       tgf-beta_receptor_inhibitor  0.999730  0.807189  0.002045\n12   aldehyde_dehydrogenase_inhibitor  0.999160  0.100000  0.001140\n36               atr_kinase_inhibitor  0.997480  0.072024  0.002295\n\nBottom 5 performing targets (by ROC-AUC):\n                                     target   roc_auc    pr_auc  log_loss\n14                           ampk_activator  0.055661  0.000221  0.004224\n57   catechol_o_methyltransferase_inhibitor  0.066898  0.000223  0.004091\n115                                laxative  0.107203  0.000118  0.002220\n15                                analgesic  0.129147  0.000121  0.002381\n118                        lipase_inhibitor  0.158757  0.000125  0.002306\n\nTotal execution time: 13.17 minutes\n","output_type":"stream"}],"execution_count":19}]}